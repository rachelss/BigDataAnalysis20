---
title: "Shell"
author: "Rachel Schwartz"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Intro

Read the [Software Carpenty Background](http://swcarpentry.github.io/shell-novice/) and [Intro](http://swcarpentry.github.io/shell-novice/01-intro/index.html).

Just as we wanted to be able to repeat commands automatically and keep track of our process in R using a script, we'd like to be able to do this when we work with files generally.
In cases where we have many files we need move those files around
and run analyses on each of them.
You might be used to doing this point-and-click, but here we'll automate the process.

This material follows the Software Carpentry Shell lessons.

## Basic commands

Before we get to writing scripts we're going to start by
typing on the command line. 
We'll do this from the Terminal tab in the lower left.

First, let's list the files in our main folder.

```{sh ls1, eval = FALSE}
ls
```

You can also specify the folder you want to list files from.

```{sh lshome, eval = FALSE}
ls ~/
```

The ~ specifies your home folder.

Now let's take a look at some data.
In this case we have more temperature data from
lots of sites in lots of sampling periods.
That means we have many files!
Rather than copying these for each of you I've put them in a shared folder
on the server so we'll list the contents of that folder.
To list the contents of a folder you need to know where it is.
In this case the shared folder is in the same directory as your home folder.
That means to access it you need to tell the computer
look in the folder that's "outside" of the current folder
then look inside the shared folder then look inside the
data folder, which is named NEON_temp-air-buoy.
We specify the "outside" folder using .. then add a / then the name of the folder we want to look in.

```{sh ls, eval=FALSE}
ls ../shared/NEON_temp-air-buoy
```

Now you see folders inside this folder.
But we're just listing the files and folders in that folder.
Let's change directory and move to that folder so we can work
with those files.

```{sh cd, eval=FALSE}
cd ../shared/NEON_temp-air-buoy
```

Challenge: list the folders in this folder.

```{sh}
ls NEON_temp-air-buoy
```

Challenge: cd into the first folder and look at all the files.

```{sh c1, eval = FALSE}
cd NEON.D03.BARC.DP1.20046.001.2019-01.basic.20190424T185418Z/
ls
```

```{sh l1, echo = FALSE}
ls NEON_temp-air-buoy/NEON.D03.BARC.DP1.20046.001.2019-01.basic.20190424T185418Z/
```

You see several csv files as well as a readme.
Let's start with the readme so you know what information you might have.
If you want to see the whole file you can use `cat` to print it out.

```{sh c2, eval = FALSE}
cat NEON.D03.BARC.DP1.20046.001.readme.20190424T185418Z.txt
```

This is quite long and hard to read so let's look at it a different way.
If you use `less` you can view part of the file at a time and scroll around.
Note: type `q` to get back to your main prompt.

```{sh c3, eval = FALSE}
less NEON.D03.BARC.DP1.20046.001.readme.20190424T185418Z.txt
```

```{sh l3, echo = FALSE}
head NEON_temp-air-buoy/NEON.D03.BARC.DP1.20046.001.2019-01.basic.20190424T185418Z/NEON.D03.BARC.DP1.20046.001.readme.20190424T185418Z.txt
```

Because we are working within the RStudio IDE you also have the option of looking at file locations in the Files pane in the lower right, and viewing files in the main scripting area.
In cases where RStudio is not installed on the server you would not have this option.

Now you have a sense that these datasets are air temperature collected every minute.
We anticipate a lot of data so let's look at the first few lines of a file using `head`.

```{sh c4, eval = FALSE}
head NEON.D03.BARC.DP1.20046.001.103.100.001.RHbuoy_1min.2019-01.basic.20190424T185418Z.csv
```

```{sh l4, echo = FALSE}
head NEON_temp-air-buoy/NEON.D03.BARC.DP1.20046.001.2019-01.basic.20190424T185418Z/NEON.D03.BARC.DP1.20046.001.103.100.001.RHbuoy_1min.2019-01.basic.20190424T185418Z.csv
```

If all we had wanted to see was the header information we can add a "flag" to the `head` command
to specify the number of lines to view.

```{sh c5, eval = FALSE}
head -1 NEON.D03.BARC.DP1.20046.001.103.100.001.RHbuoy_1min.2019-01.basic.20190424T185418Z.csv
```

```{sh l5, echo = FALSE}
head -1 NEON_temp-air-buoy/NEON.D03.BARC.DP1.20046.001.2019-01.basic.20190424T185418Z/NEON.D03.BARC.DP1.20046.001.103.100.001.RHbuoy_1min.2019-01.basic.20190424T185418Z.csv
```

We can also see the overall number of lines (and words) in the file using `wc` (for word count).

```{sh c6, eval = FALSE}
wc NEON.D03.BARC.DP1.20046.001.103.100.001.RHbuoy_1min.2019-01.basic.20190424T185418Z.csv
```

```{sh l6, echo = FALSE}
wc NEON_temp-air-buoy/NEON.D03.BARC.DP1.20046.001.2019-01.basic.20190424T185418Z/NEON.D03.BARC.DP1.20046.001.103.100.001.RHbuoy_1min.2019-01.basic.20190424T185418Z.csv
```

If we just want line count we use the `l` flag.

```{sh c7, eval = FALSE}
wc -l NEON.D03.BARC.DP1.20046.001.103.100.001.RHbuoy_1min.2019-01.basic.20190424T185418Z.csv
```

```{sh l7, echo = FALSE}
wc -l NEON_temp-air-buoy/NEON.D03.BARC.DP1.20046.001.2019-01.basic.20190424T185418Z/NEON.D03.BARC.DP1.20046.001.103.100.001.RHbuoy_1min.2019-01.basic.20190424T185418Z.csv
```

If you want to know more about the `wc` command check the manual (`man`).

```{sh man, eval = FALSE}
man wc
```

Go back to your home folder now.

```{sh c8, eval = FALSE}
cd ~
```

Let's make a new folder to put our results from this analysis using `mkdir` which means make directory.

```{sh c9, eval = FALSE}
mkdir NEON_buoy_temp_results
```

View the files in your current folder to see that this folder has been created.

```{sh c10, eval = FALSE}
ls
```

Now `cd` into this folder.

Now let's make a new file - in this case a readme for this folder.
There are text editors built in to your terminal.
We'll use `nano`.

```{sh c11, eval = FALSE}
nano readme
```

You can type some text here.
At the bottom of the screen are the commands to use nano.
To save and exit use control-X.
Now view your file in the ways we have discussed previously.

Notice your file extensions.
We have used .csv and .R and .Rmd files previously.
The extension allows the computer to automatically infer the type of file.

Let's rename our readme as readme.txt.
There's no rename command so we actually move the file to the new name using the move command `mv`.
We specify first the old name then the new name.

```{sh c12, eval = FALSE}
mv readme readme.txt
```

You need to be careful with the move command as if you move a file to a location that exists
the old file will overwrite the new file and there is no way to recover.

Make a new file with some text.
List the files in the folder. 
Rename the new file readme.txt.
List the files in the folder. 
You can see how the original readme file has been overwritten and permanently erased.

You can also copy files from location to location or make a copy in the same folder.
Make a copy of the readme file using `cp`.
As with `mv` give the source and destination.

```{sh c13, eval = FALSE}
cp readme.txt readme
```

Now view the contents of your folder.

Now copy a NEON file from the shared folder to your project folder.
You will need to give the path relative to your current folder to access the file.
If you want to use the same name you don't need to specify the name.
If you want to use the current folder as the destination just put `.`

```{sh c14, eval = FALSE}
cp ../../shared/NEON_temp-air-buoy/NEON.D03.BARC.DP1.20046.001.2019-01.basic.20190424T185418Z/NEON.D03.BARC.DP1.20046.001.103.100.001.RHbuoy_1min.2019-01.basic.20190424T185418Z.csv .
```

The `cp` command takes a source file and a destination.
You can also copy multiple files by listing them all prior to the destination folder.
Keep in mind that in this case the destination must be a folder as multiple files can't have a single
file as a destination.
Try this out with some of the NEON files.

Now let's copy the whole folder.
Before we do this let's delete the individual files we copied (so we don't duplicate anything).
Before you remove a file make sure you want to do this.
Deleted files can't be recovered.

```{sh c15, eval = FALSE}
rm NEON.D03.BARC.DP1.20046.001.103.100.001.RHbuoy_1min.2019-01.basic.20190424T185418Z.csv
```

To copy a whole folder you can still use `cp` but to include all of its contents you need to copy files in the folder recursively using `-r`.

```{sh c16, eval = FALSE}
cp -r ../../shared/NEON_temp-air-buoy/NEON.D03.BARC.DP1.20046.001.2019-01.basic.20190424T185418Z/ .
```

Now list the contents of your current folder and of the newly copied folder.

Let's copy more files for practice.
What if you want to copy all the pdf files to your folder.
You could list them all individually, but that takes a lot of writing and you could miss some.
Consider the general pattern of what we want to do:
we want to copy a file if it has any set of characters followed by pdf.
We generalize "any set of characters" using *.

```{sh c17, eval = FALSE}
cp *pdf .
```

Depending on what files are in the folder we might need to be more specific.
For example you could copy all files that start with NEON and end in pdf.

```{sh c18, eval = FALSE}
cp NEON*pdf .
```

Or even copy files that end in A-or-B-dot-pdf

```{sh c18b, eval = FALSE}
cp *[AB].pdf .
```

Note that when the shell sees a wildcard, it expands the wildcard to create a list of matching filenames before running the command that was asked for. So the previous command is the same as 

```{sh c19, eval = FALSE}
cp NEON.DOC.000783vA.pdf NEON.DOC.001113vA.pdf NEON.DOC.001152vA.pdf NEON.DOC.004613vB.pdf NEON.DOC.011081vB.pdf .
```

In some cases you want to make sure you are doing the right thing before you run the command.
With wildcards you might be confused as to how the shell is doing the expansion.
In this case you can print out the full command by "echoing" it.

```{sh c19b, eval = FALSE}
echo cp ../shared/NEON*/*[AB].pdf .
```

## Pipes and filters

As we did in R, we can pipe the output of one command directly to the next command.
First let's look at output step-by-step.

Get the line counts for the csv files in the folder of data we've been working with.

```{sh 20, eval = FALSE}
wc -l *csv
```

Now let's write the output to a file

```{sh 20b, eval = FALSE}
wc -l *csv > ~/NEON_temp/csv_line_counts.txt
```

Now let's sort this list by the file lengths.

```{sh 20c, eval = FALSE}
sort csv_line_counts.txt
```

If this doesn't sort correctly, the sort function may be looking at each character one
at a time rather than viewing the whole number.
To correct this use the `n` flag to indicate sorting by number.

```{sh 20d, eval = FALSE}
sort -n csv_line_counts.txt
```

Now we could save this to an output file as well.

```{sh 20d2, eval = FALSE}
sort -n csv_line_counts.txt > ~/NEON_temp/csv_line_counts_sorted.txt
```

Instead of doing this in two steps let's pipe the output of `wc` directly to `sort`.

```{sh 20e, eval = FALSE}
wc -l *csv | sort -n > ~/NEON_temp/csv_line_counts_sorted.txt
```

Now if you only want one column you can access that with the `cut` command.
Cut has flags to denote which columns (fields) you'd like to include (-f)
and the character you'd like to separate (delimit) the columns by (-d).
Let's try getting just the first column of date-time in our data.
Remember this is a large file so be sure to pipe to head to avoid outputting too much information.

```{sh 21, eval = FALSE}
cut -f 1 -d ',' NEON.D03.BARC.DP1.20046.001.103.100.001.RHbuoy_1min.2019-01.basic.20190424T185418Z.csv | head
```

Challenge: In the shared folder is a file named `ena_results.txt`.
This file contains some information about available data in a database.
You don't need to worry about the data, but your goal is to determine the number of 
samples in the folder, the number of species (found in the scientific_name column),
and the number of genera (that's the first half of the scientific_name column, which consists
of genus and species names).

```{sh 22, eval = FALSE}
wc -l ena_results.txt
cut -f 6 ena_results.txt | sort -u | wc -l
cut -f 6 ena_results.txt | sort -u | cut -f 1 -d ' ' | sort -u | wc -l
```

If we had wanted to get a tally of the number of each species we can do that using the `uniq` (unique)
function with -c.

```{sh 22b, eval = FALSE}
cut -f 6 ena_results.txt | sort | uniq -c
```

## Loops

Our larger goal is to be able to analyze each data file in each folder.
There are 18 folders so you don't want to run your command on each one separately.
Instead we'll loop through the files.
Bash follows the syntax below.

<p style="color:blue">
When the shell sees the keyword for, it knows to repeat a command (or group of commands) once for each item in a list. Each time the loop runs (called an iteration), an item in the list is assigned in sequence to the variable, and the commands inside the loop are executed, before moving on to the next item in the list. Inside the loop, we call for the variable’s value by putting `$` in front of it. The `$` tells the shell interpreter to treat the variable as a variable name and substitute its value in its place, rather than treat it as text or an external command.
</p>

```{sh 23, eval = FALSE}
for thing in list_of_things
do
    operation_using $thing    # Indentation within the loop is not required, but aids legibility
done
```

Let's work with a simple example first.
This will print the first line of each file.

<p style="color:blue">
In this example, the list is all filenames ending in csv. 
Remember that the shell expands the wildcard first, which means you have a specific list to loop through.
Each time the loop iterates, it will assign a file name to the variable filename and run the head command. The first time through the loop, `$filename` is the name of the first file. The interpreter runs the command head on this file. For the second iteration, `$filename` is the name of the second file. This time, the shell runs head on this file. When the end of the list is reached, the shell exits the for loop.
</p>

```{sh 24, eval = FALSE}
for filename in *csv
do
    head -1 $filename
done
```

<p style="color:blue">
Notice that the shell prompt changes from `$` to `>` and back again as we were typing in our loop. The second prompt, is different to remind us that we haven’t finished typing a complete command yet.
Here we see > being used a shell prompt, whereas > is also used to redirect output. Similarly, `$` is used as a shell prompt, but, as we saw earlier, it is also used to ask the shell to get the value of a variable. If the shell prints > or `$` then it expects you to type something, and the symbol is a prompt. If you type > or `$` yourself, it is an instruction from you that the shell should redirect output or get the value of a variable.
</p>

<p style="color:blue">
When using variables it is also possible to put the names into curly braces to clearly delimit the variable name: `$filename` is equivalent to `${filename}`, but is different from `${file}name`. You may find this notation in other people’s programs.
We have called the variable in this loop filename in order to make its purpose clearer to human readers. The shell itself doesn’t care what the variable is called, but you should think of variable names that will be clear to you or another reader.
</p>

Challenge: compare the output of the loop above to the following

```{sh 24b, eval = FALSE}
for filename in *csv
do
    head -1 ${filename} *csv
done
```

You might notice that if you hit the up arrow to repeat a loop the command is compressed into one line. To decipher where the line breaks should be look for the semi-colons.

Challenge: Think about the following loop. What do you expect it to do?
Now run it and see if the result matches your expectation.

```{sh 25, eval = FALSE}
for filename in *csv
do
    echo $filename
    head -1 $filename > ~/NEON_temp/csv_headers.csv
done
```

Note that each time you run the loop you overwrite the file.
`>>` appends to a file, rather than overwriting it with the redirected output from a command.

```{sh 25b, eval = FALSE}
for filename in *csv
do
    echo $filename
    head -1 $filename >> ~/NEON_temp/csv_headers.csv
done
```

Loops are particularly useful when trying to rename files.
Challenge: what do you think the following would do.

```{sh 25c, eval = FALSE}
cp *csv original*csv
```

We assume that the person implementing this command wants to
have two copies of their files - one they are working with and one marked original.
This is great practice if you are thinking about modifying files.
However, here the wildcard expansion means that the shell thinks you are trying to 
copy the list of csv files (expanded) to "original*csv".
Remember it can't find a file of this name so there's no expansion and it will be treated as the destination.

Instead, we can use a loop:

```{sh 26, eval = FALSE}
for filename in *csv
do
    echo $filename
    cp $filename original_$filename
done
```

Before we run this let's see how the commands will be run.

```{sh 26b, eval = FALSE}
for filename in *csv
do
    echo $filename
    echo cp $filename original_$filename
done
```

Now let's go back to our original problem.
You need to
1. Copy the csv files for 1 minute air temperature into a folder in your directory
2. Run the 30-minute-averaging procedure on each file. (The command to run the script is in the first lines of the file)

```{sh 27, eval = FALSE}
for filename in *csv
do
    echo $filename
    Rscript 30min_avg_neon.R $filename
done
```

## Scripts

To preserve the code and make it reproducible, copy it into a script.
Inside the terminal you can use `nano` or if you're using RStudio server you can make a new file in the script window
and paste your code there.
Save the script with a `.sh` extension to indicate that it is a shell script.
Now save the file and exit nano if necessary.
Run the shell script by typing 

```{sh 28, eval = FALSE}
bash 30min_avg_neon.sh
```

To help future readers of this script know what it does and how to run it add some comments at the top.

```{sh 28b, eval = FALSE}
# Run RScript to get 30min averages for neon data for all csvs
# Usage: bash 30min_avg_neon.sh
```

This works, but every time we want to change the list of files we're analyzing
we need to change the script.
Alternatively, we can specify arguments on the command line.
We can then pass these arguments to the script.
Let's start with a simple example.
Write a script to print the 10th line of the first csv file.

```{sh 29, eval = FALSE}
# Print the 10th line of the file.
# Usage: bash printline10.sh

head -10 NEON.D03.BARC.DP1.20046.001.103.100.001.RHbuoy_1min.2019-01.basic.20190424T185418Z.csv | tail -1
```

Now let's change this so we can specify the name of the file on the command line
rather than opening the file every time.
In your text editor replace the file name with the special variable `$1`.
Inside a shell script, $1 means 'the first argument on the command line'. 
Note that you should put $1 with double-quotes in case the filename happens to contain any spaces.

```{sh 29bb, eval = FALSE}
# Print the 10th line of the file.
# Usage: bash printline10.sh

head -10 "$1" | tail -1
```

We can now run our script like this:

```{sh 29c, eval = FALSE}
bash printline10.sh NEON.D03.BARC.DP1.20046.001.103.100.001.RHbuoy_1min.2019-01.basic.20190424T185418Z.csv
```

Or like this:

```{sh 29d, eval = FALSE}
bash printline10.sh NEON.D03.BARC.DP1.20046.001.103.100.001.RHbuoy_1min.2019-02.basic.20190418T193559Z.csv
```

If we want to pick a different line to print we currently need to edit the file.
Let’s fix that by using the special variable `$2` for the number of the lines to be passed to head:

```{sh 29b, eval = FALSE}
# Print a specified line of a file.
# Usage: bash printline10.sh [filename] [line to print]

head -"$2" "$1" | tail -1
```

Now we run the script as

```{sh 29d2, eval = FALSE}
mv printline10.sh printline.sh
bash printline.sh NEON.D03.BARC.DP1.20046.001.103.100.001.RHbuoy_1min.2019-02.basic.20190418T193559Z.csv 10
```

Note that you should change the file name, description of what it does, and usage.

Now let's go back to our original script to loop through our csv files and calculate 30 minute averages.
If we want to specify the list of files on the command line 
we can’t use `$1`, `$2`, and so on because we don’t know how many files there are. Instead, we use the special variable `$@`, which means, 'All of the command-line arguments to the shell script'. 
We also should put `$@` inside double-quotes to handle the case of arguments containing spaces.
Remember that when we put `*csv` as a command line argument the wildcard is expanded before passing arguments
to the script so you are passing all the file names individually, not the single argument.
If you want to pass '*csv' literally without expanding it we need to enclose it in single quote marks. 
The variable `$1` will then be '*csv' and can be expanded within the script.

```{sh 30, eval = FALSE}
# Run RScript to get 30min averages for neon data for all csvs
# Usage: bash 30min_avg_neon.sh [filenames]

for filename in "$@"
do
    echo $filename
    Rscript 30min_avg_neon.R $filename
done
```

```{sh 30b, eval = FALSE}
bash 30min_avg_neon.sh *csv
```

```{sh 30c, eval = FALSE}
bash 30min_avg_neon.sh NEON.D03.BARC.DP1.20046.001.2019-02.basic.20190418T193559Z/*csv
```

## Finding things

### Finding things in files

We have downloaded data for a bunch of different sites.
The readme file in each folder provides the latitude and longitude of each site.
If we wanted to put this information into a single metadata spreadsheet to work with we could open each file and copy-paste the coordinates.
Or we can ask the computer to do the searching for us.

`grep` finds and prints lines in files that match a pattern. 
grep is a contraction of ‘global/regular expression/print’.
If our files we are interested in finding lines containing "Geographic coordinates"
and printing those to the screen or an output file.
To use `grep` type `grep`, then the pattern we’re searching for and finally the name of the file (or files) we’re searching in.

```{sh 31, eval = FALSE}
grep Geographic */*readme*
```

The output is the line(s) in each file that contain the search term.
By default, grep searches for a pattern in a case-sensitive way. 
Use the option -i to make the search case-insensitive.
In addition, the search pattern does not have to form a complete word, as we will see in the next example.

```{sh 31b, eval = FALSE}
grep "Geo" */*readme*
```

To restrict matches to lines containing the search term as a complete work, we can give grep with the -w option. 
This will limit matches to word boundaries, which doesn't match anything in this example.

If you want to search for a phrase rather than a single word, put the phrase in quotes.

```{sh 31c, eval = FALSE}
grep "Geographic coordinates" */*readme*
```

grep’s real power comes from the fact that search patterns can include wildcards (termed regular expressions).
Let's say we want to find measurements on April 20, 2019 at 1AM.
First we tell grep to anchor its search at the beginning of the line with ^.
Then we want a quote, but we've been using quote indicate what to search for so to avoid ending 
our search term we need to indicate that we're search for this character by escaping it.
The backslash -- \ -- escapes a special character, which means that character gets interpreted literally (and is therefore no longer special).
Now we can enter the rest of our search string.

```{sh 32, eval = FALSE}
grep "^\"2019-04-20T01:00" */*csv
```

If we want to search for measurements on April 20 OR 21, 2019 at 1AM
we need to indicate either a 0 or 1.
Brackets -- [...] -- enclose a set of characters to match in a single regular expression.

```{sh 3b2, eval = FALSE}
grep "^\"2019-04-2[01]T01:00" */*csv
```

[0-9] matches any single digit.
We can use this to get measurements any time from April 20-29, 2019 at 1AM.

```{sh 32c, eval = FALSE}
grep "^\"2019-04-2[0-9]T01:00" */*csv
```

Parentheses enclose longer sets of alternative strings and are used with the or operator "|".
Here we get measurements any time on April 1, 2019 or 2020 at 1AM.

```{sh 32d, eval = FALSE}
grep -E "^\"20(19|20)-04-01T01:00" */*csv
```

Note that as we get more complicated regular expressions we add the E flag to support this.

Now you try.
Search for temperature measurements at other times.
And of course try `man grep` to view other flags and options for searching.

### Finding files

<p style="color:blue">
While grep finds lines in files, the find command finds files themselves. 
For our first command, let’s run `find .`
</p>

```{sh 33, eval = FALSE}
find .
```

<p style="color:blue">
As always, the . on its own means the current working directory, which is where we want our search to start. `find`'s output is the names of every file and directory under the current working directory. 
</p>

We can specify that we want to find specifically files or directories using the `-type` flag.

```{sh 33b, eval = FALSE}
find . -type f
```

```{sh 33c, eval = FALSE}
find . -type d
```

Now let’s try matching by name:

```{sh 34, eval = FALSE}
find . -name *.txt
```

Although this seems to work okay, remember that the command line generally expands a wildcard before running a command.
To ensure that we are finding '*.txt' not the txt files in the current directory only,
we should put the search term in quotes.

```{sh 34b, eval = FALSE}
find . -name '*.txt'
```

If you wanted the line counts of all these files you might try to pipe this output to `wc`.

```{sh 35, eval = FALSE}
find . -name '*.txt' | wc -l
```

However, this counts the number of lines in the output not in each file.
What we want to do is the following

```{sh 35b, eval = FALSE}
wc -l [the output of the following command: find . -name '*.txt']
```

We do this by puting the find command inside `$()`.

```{sh 35c, eval = FALSE}
wc -l $(find . -name '*.txt')
```

<p style="color:blue">
When the shell executes this command, the first thing it does is run whatever is inside the `$()`. 
It then replaces the `$()` expression with that command’s output. 
This expansion is exactly what the shell does when it expands wildcards like * and ?, but lets us use any command we want as our own ‘wildcard’.
</p>

<p style="color:blue">
It’s very common to use find and grep together. 
The first finds files that match a pattern; the second looks for lines inside those files that match another pattern. 
The find command can be given several other criteria known as “tests” to locate files with specific attributes, such as creation time, size, permissions, or ownership. Use `man find` to explore these.
</p>

## Variables


## Conditionals




## Permissions and executables

Previously we ran our script as

```{sh 36, eval = FALSE}
bash 30min_avg_neon.sh *csv
```

If you want to skip the `bash` instruction and make your script into an executable accessible to anyone you need to change who has 
access to the script and what kind of access they have.
Take a look at your files.

```{sh 36b, eval = FALSE}
ls -lh
```

<p style="color:blue">
The -l flag tells ls to give us a long-form listing. It’s a lot of information, so let’s go through the columns in turn.
On the right side, we have the files’ names. 
Next to them, moving left, are the times and dates they were last modified. 
Backup systems and other tools use this information in a variety of ways, but you can use it to tell when you (or anyone else with permission) last changed a file.
Next to the modification time is the file’s size in bytes and the names of the user and group that owns it. 
We’ll skip over the second column for now (the one showing 1 for each file) because it’s the first column that we care about most. 
This shows the file’s permissions, i.e., who can read, write, or execute it.
</p>

<p style="color:blue">
Let’s have a closer look at one of those permission strings: for example `-rwxr-xr-x`. 
The first character tells us what type of thing this is: ‘-‘ means it’s a regular file, while ‘d’ means it’s a directory, and other characters mean more esoteric things.
The next three characters tell us what permissions the file’s owner has. Here, the owner can read, write, and execute the file: rwx. The middle triplet shows us the group’s permissions. If the permission is turned off, we see a dash, so r-x means “read and execute, but not write”. The final triplet shows us what everyone who isn’t the file’s owner, or in the file’s group, can do. In this case, it’s ‘r-x’ again, so everyone on the system can look at the file’s contents and run it.
</p>

To change permissions, we use the `chmod` command (whose name stands for “change mode”). 
Let’s run `chmod` to everyone give everyone ("all") read and execute, but not write permission:

```{sh 36c, eval = FALSE}
chmod a=rx 30min_avg_neon.sh
ls -lh
```

Now that we have made our script into an executable we no longer need `bash` in front of the script name to run it.
However, that means that the computer doesn't know what interpreter to use to run the script.
To inform the computer what interpreter to use add a "shebang" line as the first line of your script.
Generally you can use `#!/bin/bash` although
`#!/usr/bin/env bash` is often recommended for portability in case 
`bash` is in a location other than `/bin`.

## Sed and Awk
Text from [here](https://neurohackweek.github.io/advancedunix/01-first-part/)

### Sed

<p style="color:blue">
`sed` stands for “stream editor”. The purpose of sed is to process strings (from stdin or a file), operating on each line. Digital Ocean has a nice [tutorial on sed] (https://www.digitalocean.com/community/tutorials/the-basics-of-using-the-sed-stream-editor-to-manipulate-text-in-linux) that is far more comprehensive than what we’ll do here. For now, we’ll just discuss one of the most popular uses of sed, which is string substitution.

The general format of sed substitution is as follows: ~~~ sed ‘s/pattern/replacement/g’ ~~~ The text in single quotes is the command for sed. The first character, s is the substitution command for sed. pattern is a regular expression that is replaced by replacement. The final character, g, means that all occurrances of the pattern should be globally replaced (not just the first).
</p>

### Awk

<p style="color:blue">
Awk is a tool for processing tabular data that is named after the initials of its authors (Aho, Weinberger, and Kernighan). It is popular enough that it has been rewritten and improved, as nawk (new awk) and gawk (GNU awk). Although these variants have differences, and some scripts may work only with particular versions of gawk (for example), the general principles are the same. A full description of awk is well beyond the scope of this tutorial. Grymoire has [a nice tutorial] (http://www.grymoire.com/Unix/Awk.html), and you can also read the comprehensive [gawk manual] (https://www.gnu.org/software/gawk/manual/gawk.html).
The basic structure of an awk script is as follows:
</p>

```{sh 37, eval = FALSE}
BEGIN { action }
pattern { action }
END { action }
```

<p style="color:blue">
Basically, awk reads a text file that is structured by fields and records, and fires code (in curly braces) when records match specific patterns. This is made more powerful by the ability to do stuff before you start reading in the BEGIN action block, and the ability to stuff at the END action block.
A common use for awk is to parse comma separated value files (csv files). In a csv file, records are lines (and the record separator is a newline character) and fields are items separated by commas within a line.
</p>